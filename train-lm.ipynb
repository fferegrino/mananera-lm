{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0cccce9-0323-4421-894c-6675bc4de999",
   "metadata": {},
   "source": [
    "# Creando un (simple) modelo de lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b4228-33d9-4846-8f9d-ac3b2e4e60d7",
   "metadata": {},
   "source": [
    "## Preparación del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff21208-5fbf-4d6e-a682-ea03a051d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5572e9b-edc2-4217-bfee-752a61d3a710",
   "metadata": {},
   "source": [
    "### Función tokenizadora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaff19d-8be7-4399-8150-d471b3f42160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import string\n",
    "from typing import List\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_es = set(nltk.corpus.stopwords.words('spanish'))\n",
    "punctuation = set(string.punctuation + '¡¿…')\n",
    "\n",
    "def tokenize_dialog(dialog: str) -> List[str]:\n",
    "    tokens = []\n",
    "    for sentence in sent_tokenize(dialog):\n",
    "        for token in tokenizer.tokenize(sentence):\n",
    "            token = token.lower()\n",
    "            # if token in stopword_es:\n",
    "            #     continue\n",
    "            # if token in punctuation:\n",
    "            #     continue\n",
    "            tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c45e74a-0f13-4ce6-9278-24c98b3fed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diálogo = (\"Y también nos planteó que se atendiera a jóvenes en casas especiales para terapias y apoyo a personas \"\n",
    "           \"con discapacidad. También, ya se buscó una alternativa y ya tenemos una respuesta.\")\n",
    "\n",
    "tokens = tokenize_dialog(diálogo)\n",
    "\n",
    "print(\" - \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e42402-95b8-4961-b5d1-593c91bc9d72",
   "metadata": {},
   "source": [
    "### Aplicándolo a los diálogos de las conferencias presidenciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa071cf-cefc-4c54-9f86-3d179c4abba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "dialogos = []\n",
    "\n",
    "with gzip.open(\"conferencias.txt.gz\", 'rt') as r:\n",
    "    for line in r:\n",
    "        dialogos.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb638fa-b37f-44a5-8dc5-ba203255dab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tokens = [\n",
    "    tokenize_dialog(dialogo) for dialogo in dialogos\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda0067-001b-4b50-9be2-0801142cb158",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6527d-fb5d-4cd2-92c8-9092cf90c796",
   "metadata": {},
   "source": [
    "## Train a new Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac9302-7c70-4e00-9872-98912ae0359a",
   "metadata": {},
   "source": [
    "## Clase `Vocabulary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ff0c2-1d7d-4285-bbff-779a75aaee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from typing import Optional, List\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    START_TOKEN = \"<p>\"\n",
    "    END_TOKEN = \"</p>\"\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "    def __init__(self, size: Optional[int] = None):\n",
    "        self.size = size\n",
    "\n",
    "    def fit(self, tokenized_texts: List[List[str]]) -> \"Vocabulary\":\n",
    "        # Creamos una sola lista con todos los tokens\n",
    "        tokens = chain.from_iterable(tokenized_texts)\n",
    "\n",
    "        # Contamos las ocurrencias de cada token\n",
    "        self.single_token_counts = Counter(tokens)\n",
    "        self.total_number_of_tokens = sum(self.single_token_counts.values())\n",
    "\n",
    "        # Obtenemos los tokens más comunes con `most_common`, si `size` es `None` entonces se obtienen todos los tokens\n",
    "        # Si `size` no es `None` entonces se obtienen los `size` tokens más comunes menos 3 (por los tokens especiales)\n",
    "        top_counts = self.single_token_counts.most_common(None if self.size is None else (self.size - 3))\n",
    "\n",
    "        # Creamos el vocabulario con los tokens especiales y los tokens más comunes\n",
    "        vocab = [self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] + [w for w, _ in top_counts]\n",
    "\n",
    "        # Creamos los diccionarios para convertir de id a palabra y viceversa\n",
    "        self.id_to_word = dict(enumerate(vocab))\n",
    "        self.word_to_id = {v: k for k, v in iter(self.id_to_word.items())}\n",
    "\n",
    "        # Establecemos estas variables para acceder más fácilmente a la cantidad de tokens y al tamaño del vocabulario\n",
    "        self.size = len(self.id_to_word)\n",
    "        self.tokenset = set(self.word_to_id.keys())\n",
    "\n",
    "        # Almacenamos los ids de los tokens especiales\n",
    "        self.start_id = self.word_to_id[self.START_TOKEN]\n",
    "        self.end_id = self.word_to_id[self.END_TOKEN]\n",
    "        self.unk_id = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, tokenized_texts: List[List[str]]):\n",
    "        padded_posts = ([self.START_TOKEN, self.START_TOKEN] + p + [self.END_TOKEN] for p in tokenized_texts)\n",
    "        cannonical_posts = [[self.word_to_id.get(w, self.unk_id) for w in p] for p in padded_posts]\n",
    "        return cannonical_posts\n",
    "\n",
    "    def words_to_ids(self, words: List[str]) -> List[int]:\n",
    "        return [self.word_to_id.get(w, self.unk_id) for w in words]\n",
    "\n",
    "    def ids_to_words(self, ids: List[int]) -> List[str]:\n",
    "        return [self.id_to_word[i] for i in ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfdb017-05c6-4b95-b993-09a347d40c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.fit(training_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772a278-cdfd-41b2-92f1-6d7f120020e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(vocab.tokenset)[:10])\n",
    "vocab.total_number_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5073f-77e3-47ca-a74d-068d6d46bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.single_token_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87672b2-dc9b-470c-bc81-f57b897399db",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = vocab.transform([['hola', 'cómo', 'están']])\n",
    "print(\"Ids:\", transformed)\n",
    "words = vocab.ids_to_words(transformed[0])\n",
    "print(\"Words:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8f9eb-64de-4c13-b580-5dcab5874a94",
   "metadata": {},
   "source": [
    "## Crea un nuevo modelo de lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2be7d3-5d13-43e3-9062-e89d9dfcbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class AddKTrigramLM:\n",
    "    \"\"\"Modelo de lenguaje de trigramas con suavizado Add-k.\"\"\"\n",
    "\n",
    "    def __init__(self, k: float = 0.0):\n",
    "        \"\"\"Inicializa el modelo de lenguaje con el valor de k.\"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, corpus: List[List[str]]) -> \"AddKTrigramLM\":\n",
    "        \"\"\"Entrena el modelo de lenguaje a partir de un corpus.\"\"\"\n",
    "\n",
    "        # Creamos un diccionario de diccionarios para guardar las cuentas de los trigramas\n",
    "        # En este diccionario las ocurrencias del token `w` dado `w_1 w_2` se almacenan como `counts[(w_2,w_1)][w]`\n",
    "        counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "        # Necesitamos tener una lista de todos los tokens en el corpus\n",
    "        # Esto es necesario para calcular el tamaño del vocabulario\n",
    "        # y para calcular las probabilidades de los tokens\n",
    "        all_tokens = set()\n",
    "\n",
    "        # Iteramos sobre cada documento del corpus\n",
    "        # Para cada documento iteramos sobre cada token\n",
    "        # En cada iteración actualizamos el contexto y las cuentas de los trigramas\n",
    "        w_1, w_2 = None, None\n",
    "        for document in corpus:\n",
    "            for token in document:\n",
    "                all_tokens.add(token)\n",
    "                if w_1 is not None and w_2 is not None:\n",
    "                    counts[(w_2, w_1)][token] += 1\n",
    "                # Update context\n",
    "                w_2 = w_1\n",
    "                w_1 = token\n",
    "\n",
    "        # Convertir los defaultdicts en diccionarios normales\n",
    "        # esto es solo por fines de presentación\n",
    "        self.token_totals = dict()\n",
    "        for context, ctr in counts.items():\n",
    "            self.token_totals[context] = dict(ctr)\n",
    "\n",
    "        # En `context_totals` almacenamos las ocurrencias de los bigramas `w_2,w_1`\n",
    "        self.context_totals = dict()\n",
    "        for context, ctr in counts.items():\n",
    "            self.context_totals[context] = sum(ctr.values())\n",
    "\n",
    "        self.tokens = list(all_tokens)\n",
    "        self.V = len(self.tokens)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def set_k(self, k: float = 0.0) -> None:\n",
    "        self.k = k\n",
    "\n",
    "    def next_token_proba(self, token: str, current_sequence: List[str]) -> float:\n",
    "        \"\"\"Calcula la probabilidad de un token dada una secuencia de tokens.\"\"\"\n",
    "\n",
    "        # Obtenemos los 2 últimos tokens de la secuencia: `w_1` y `w_2`\n",
    "        context = tuple(current_sequence[-2:])\n",
    "\n",
    "        # Count word es la cuenta de las veces que ocurren los tokens `w_1`, `w_2` y `token` en el corpus\n",
    "        count_word = self.token_totals.get(context, {}).get(token, 0)\n",
    "\n",
    "        # Context count es la cuenta de las veces que ocurren los tokens `w_1` y `w_2` en el corpus\n",
    "        context_count = self.context_totals.get(context, 0)\n",
    "\n",
    "        if self.k == 0:\n",
    "            # Si k = 0, entonces no se aplica suavizado y la probabilidad es la división de la cuenta de los\n",
    "            # tokens `w_1`, `w_2` y `token` entre la cuenta de los tokens `w_1` y `w_2`\n",
    "            return count_word / context_count\n",
    "        else:\n",
    "            # El cáculo de la probabilidad es la división de la cuenta de los tokens `w_1`, `w_2` y `token` entre la\n",
    "            # cuenta de los tokens `w_1` y `w_2`\n",
    "            return (count_word + self.k) / (context_count + self.k * self.V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142dd50-0f0a-499b-9aa9-1d77733cb987",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = AddKTrigramLM()\n",
    "\n",
    "lm.fit(vocab.transform(training_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794c1fb-0553-4342-a676-3ad0568178b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inicial = 'hola'\n",
    "id_inicial = vocab.word_to_id[token_inicial]\n",
    "lm.token_totals[(vocab.start_id, vocab.start_id)][id_inicial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff19ea-6d1d-4d36-9d07-afeb89eac93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.next_token_proba(id_inicial, [vocab.start_id,vocab.start_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae058f-a5be-408d-857e-230eeb83301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from add_k_trigram_lm import AddKTrigramLM\n",
    "from vocabulary import Vocabulary\n",
    "\n",
    "\n",
    "class SequenceGenerator:\n",
    "    def __init__(self, language_model: AddKTrigramLM, vocabulary: Vocabulary):\n",
    "        self.lm = language_model\n",
    "        self.vocab = vocabulary\n",
    "\n",
    "    def sample_next_token(self, *sequence: Tuple[str]) -> str:\n",
    "        # Esto busca cada palabra en el vocabulario y obtiene su probabilidad condicional.\n",
    "        # Esto puede ser lento si el vocabulario es muy grande, ¿podríamos hacerlo mejor?\n",
    "        probs = [self.lm.next_token_proba(word, sequence) for word in self.lm.tokens]\n",
    "\n",
    "        # Elegimos una palabra al azar de acuerdo a sus probabilidades\n",
    "        return np.random.choice(self.lm.tokens, p=probs)\n",
    "\n",
    "    def generate_sequences(self, *start: Tuple[str], max_length: int = 200) -> str:\n",
    "        # Given it the start sequence to indicate the start of a post.\n",
    "        seq = [self.vocab.start_id, self.vocab.start_id]\n",
    "        if start:\n",
    "            start = [self.vocab.word_to_id[w] for w in start]\n",
    "            seq.extend(start)\n",
    "        for i in range(max_length):\n",
    "            seq.append(self.sample_next_token(*seq))\n",
    "            # Stop at post\n",
    "            if seq[-1] == self.vocab.end_id:\n",
    "                break\n",
    "        return \" \".join([f\"{self.vocab.id_to_word[s]}\" for s in seq])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb25b3-9987-42f8-883a-cc210d134297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm.set_k(1e-3)\n",
    "lm.set_k(0)\n",
    "generador = SequenceGenerator(lm, vocab)\n",
    "start = ['el', 'pueblo']\n",
    "for _ in range(5):\n",
    "    print(generador.generate_sequences(*start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa80a3c-4f9f-43aa-89f3-7ae9d75e291a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
